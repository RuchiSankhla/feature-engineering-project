# üß© Feature Engineering Project ‚Äî Employee Attrition Prediction

## üìò Project Overview
This project focuses on performing **Feature Engineering** on the **Employee Attrition Prediction Dataset** obtained from Kaggle.  
The goal of this project is to clean, preprocess, and transform raw HR data into a structured format suitable for predictive modeling.  
All the tasks were carried out using **Python** in **Jupyter Notebook**.

---

## üéØ Objectives
- To handle missing, duplicate, and noisy data.  
- To detect and treat outliers.  
- To encode categorical variables for machine learning compatibility.  
- To scale and normalize numerical features.  
- To create and select meaningful features for better model performance.

---

## ‚öôÔ∏è Tools & Libraries Used
- Python  
- Pandas  
- NumPy  
- Matplotlib  
- Seaborn  
- Scikit-learn  

---

## üìÅ Folder Structure

feature-engineering-project/
‚îÇ
‚îú‚îÄ‚îÄ feature_engineering.ipynb # Main notebook with all code and outputs
‚îú‚îÄ‚îÄ theory_report.pdf # Theory report or documentation
‚îú‚îÄ‚îÄ README.md # Project overview (this file)
‚îî‚îÄ‚îÄ data/ # Dataset files 


---

## üß† Steps Performed

### 1Ô∏è‚É£ Data Cleaning  
- Removed irrelevant columns such as *Employee_ID*.  
- Handled missing values using mean, median, and mode imputation.  
- Treated noisy data (e.g., unrealistic values like age > 100).  
- Removed duplicate records.

### 2Ô∏è‚É£ Data Encoding  
- Converted categorical columns using **Label Encoding** and **One-Hot Encoding**.

### 3Ô∏è‚É£ Feature Scaling  
- Applied **StandardScaler** and **MinMaxScaler** for normalization and standardization.

### 4Ô∏è‚É£ Feature Transformation  
- Created new derived features (like total working years, experience level, etc.).  
- Applied **binning** for continuous variables (e.g., salary categories ‚Üí Low, Medium, High).

### 5Ô∏è‚É£ Feature Selection  
- Used **Correlation Matrix** and **Feature Importance** to identify relevant features.

---

## üìä Results
After applying feature engineering techniques:
- The dataset became clean, consistent, and ready for model training.  
- Outliers were reduced, and scaling improved uniformity across features.  
- The transformed dataset is now suitable for classification models such as Logistic Regression, KNN, and Decision Tree.

---

## üßæ Files Description
| File Name | Description |
|------------|-------------|
| `feature_engineering.ipynb` | Main Jupyter notebook containing the complete analysis |
| `theory_report.pdf` | Document explaining theoretical background and observations |
| `README.md` | Overview of the entire project |
| `data/` | Folder containing dataset (if applicable) |

---

## üë©‚Äçüíª Author
**Ruchi Sankhla**  
MCA Student, Lachoo Memorial College of Science & Technology, Jodhpur  
DRDO Internship Project (2025)  
*Feature Engineering and Data Preprocessing using Python*

---

## üîó Dataset Source
**Kaggle:** [Employee Attrition Prediction Dataset](https://www.kaggle.com)  
*(You can replace this link with the exact dataset link you used.)*
